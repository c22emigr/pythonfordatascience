{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFa19CCl6LWU"
      },
      "source": [
        "# Intro To DS Course: Python for Data Science"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOR2FJG46kmF"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook provides a comprehensive introduction to pandas, numpy, matplotlib and plotly. We'll cover essential concepts and practical applications for data science.\n",
        "\n",
        "Learning Objectives:\n",
        "- Understand pandas data structures (Series and DataFrame)\n",
        "- Learn data loading, inspection, and basic operations\n",
        "- Master data cleaning and preprocessing techniques\n",
        "- Perform data manipulation and transformation\n",
        "- Conduct basic statistical analysis and aggregation\n",
        "\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "\n",
        "To get started, you need to set up your Python environment. We recommend using\n",
        "a virtual environment to manage your project dependencies. The essential libraries for this notebook are pandas, numpy, matplotlib and plotly.\n",
        "You can install these libraries using pip, the Python package installer.\n",
        "\n",
        "Run the following cell to install the required libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0BuE17p6cDN"
      },
      "outputs": [],
      "source": [
        "# Installation of necessary libraries\n",
        "!pip install pandas numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9SHIO-U7Jen"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJEF9_yc7CPt"
      },
      "outputs": [],
      "source": [
        "# check versions\n",
        "\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e14b0d5b"
      },
      "source": [
        "## Pandas basics\n",
        "\n",
        "Pandas is a fundamental library in Python for data manipulation and analysis.\n",
        "It provides two primary data structures: Series and DataFrame.\n",
        "\n",
        "**Series:**\n",
        "A Series is a one-dimensional array-like object that can hold data of any data type\n",
        "(integers, strings, floating point numbers, Python objects, etc.). It is similar\n",
        "to a column in a spreadsheet or a SQL table. Each element in a Series has an\n",
        "associated label, called an index. If no index is specified, a default integer\n",
        "index from 0 to N-1 (where N is the length of the data) is created.\n",
        "\n",
        "**DataFrame:**\n",
        "A DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous\n",
        "tabular data structure with labeled axes (rows and columns). It is similar to\n",
        "a spreadsheet, a SQL table, or a dictionary of Series objects. DataFrames are\n",
        "the most commonly used Pandas object and are ideal for representing real-world\n",
        "datasets.\n",
        "\n",
        "**Key Differences:**\n",
        "- Series are one-dimensional, while DataFrames are two-dimensional.\n",
        "- A DataFrame can be thought of as a collection of Series that share the same index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW2xqcDr7rKk"
      },
      "outputs": [],
      "source": [
        "# Creating a Pandas Series\n",
        "\n",
        "# Creating a Series from a Python list\n",
        "my_list = [10, 20, 30, 40, 50]\n",
        "series_from_list = pd.Series(my_list)\n",
        "print(\"Series from a Python list:\")\n",
        "display(series_from_list)\n",
        "\n",
        "# Creating a Series from a NumPy array\n",
        "my_array = np.array([100, 200, 300, 400, 500])\n",
        "series_from_array = pd.Series(my_array)\n",
        "print(\"\\nSeries from a NumPy array:\")\n",
        "display(series_from_array)\n",
        "\n",
        "# Creating a Series with a custom index\n",
        "custom_index_series = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n",
        "print(\"\\nSeries with a custom index:\")\n",
        "display(custom_index_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqe9Syie70XX"
      },
      "outputs": [],
      "source": [
        "# Creating a Pandas DataFrame\n",
        "\n",
        "# Creating a DataFrame from a Python dictionary\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "        'Age': [25, 30, 35, 40],\n",
        "        'City': ['New York', 'London', 'Paris', 'Tokyo']}\n",
        "dataframe_from_dict = pd.DataFrame(data)\n",
        "print(\"\\nDataFrame from a Python dictionary:\")\n",
        "display(dataframe_from_dict)\n",
        "\n",
        "# Creating a DataFrame from a list of dictionaries\n",
        "list_of_dicts = [{'Name': 'Eve', 'Age': 22, 'City': 'Rome'},\n",
        "                 {'Name': 'Frank', 'Age': 28, 'City': 'Berlin'},\n",
        "                 {'Name': 'Grace', 'Age': 33, 'City': 'Sydney'}]\n",
        "dataframe_from_list_of_dicts = pd.DataFrame(list_of_dicts)\n",
        "print(\"\\nDataFrame from a list of dictionaries:\")\n",
        "display(dataframe_from_list_of_dicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2qHFUtr74xP"
      },
      "source": [
        "## Selecting and Indexing Data\n",
        "\n",
        "Selecting and indexing data in Pandas is crucial for accessing specific\n",
        "elements or subsets of your data. Pandas provides several ways to do this\n",
        "using both integer-based and label-based indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv1GSLLW8Jud"
      },
      "outputs": [],
      "source": [
        "# Selecting data from a Series\n",
        "\n",
        "# Using integer-based indexing (position)\n",
        "print(\"\\nSelecting elements from a Series using integer-based indexing:\")\n",
        "display(series_from_list[0])\n",
        "display(series_from_list[1:4]) # Slicing\n",
        "\n",
        "# Using label-based indexing (index labels)\n",
        "print(\"\\nSelecting elements from a Series using label-based indexing:\")\n",
        "display(custom_index_series['a'])\n",
        "display(custom_index_series[['b', 'd']]) # Selecting multiple labels\n",
        "display(custom_index_series['b':'d']) # Slicing with labels (inclusive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d23dcecb"
      },
      "outputs": [],
      "source": [
        "# Selecting columns from a DataFrame\n",
        "\n",
        "print(\"\\nSelecting a single column from a DataFrame:\")\n",
        "display(dataframe_from_dict['Name'])\n",
        "display(dataframe_from_dict.Age) # Another way to select a single column\n",
        "\n",
        "print(\"\\nSelecting multiple columns from a DataFrame:\")\n",
        "display(dataframe_from_dict[['Name', 'City']])\n",
        "\n",
        "# Selecting rows from a DataFrame\n",
        "\n",
        "# Using integer-based indexing with .iloc[]\n",
        "print(\"\\nSelecting rows from a DataFrame using .iloc[] (integer-based):\")\n",
        "display(dataframe_from_dict.iloc[0]) # Select the first row\n",
        "display(dataframe_from_dict.iloc[1:3]) # Select rows from index 1 to 2 (exclusive of 3)\n",
        "\n",
        "# Using label-based indexing with .loc[]\n",
        "print(\"\\nSelecting rows from a DataFrame using .loc[] (label-based):\")\n",
        "display(dataframe_from_dict.loc[0]) # Select the row with index label 0\n",
        "display(dataframe_from_dict.loc[1:3]) # Select rows from index label 1 to 3 (inclusive of 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-656PGS8WCk"
      },
      "outputs": [],
      "source": [
        "# Selecting specific cells or subsets using combined indexing\n",
        "\n",
        "print(\"\\nSelecting a specific cell using .iloc[]:\")\n",
        "display(dataframe_from_dict.iloc[0, 0]) # Select the element at row 0, column 0\n",
        "\n",
        "print(\"\\nSelecting a specific cell using .loc[]:\")\n",
        "display(dataframe_from_dict.loc[0, 'Name']) # Select the element at row label 0, column label 'Name'\n",
        "\n",
        "print(\"\\nSelecting a subset of the DataFrame using .iloc[]:\")\n",
        "display(dataframe_from_dict.iloc[0:2, 0:2]) # Select rows 0 and 1, and columns 0 and 1\n",
        "\n",
        "print(\"\\nSelecting a subset of the DataFrame using .loc[]:\")\n",
        "display(dataframe_from_dict.loc[0:1, ['Name', 'Age']]) # Select rows with labels 0 and 1, and columns 'Name' and 'Age'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c81a551"
      },
      "source": [
        "## Data loading and inspection\n",
        "\n",
        "In real-world data science projects, data rarely comes in neatly defined\n",
        "DataFrames within your script. You'll often need to load data from external\n",
        "files. Pandas provides convenient functions to read data from various formats,\n",
        "including CSV and Excel files, which are very common.\n",
        "\n",
        "After loading data, the first crucial step is always to inspect it. This helps\n",
        "you understand the structure, data types, and get a quick overview of the\n",
        "data's content and quality. We will use methods like `.head()`, `.info()`,\n",
        "and `.describe()` for this initial inspection.\n",
        "\n",
        "Create dummy CSV and Excel files with sample data and then load them into pandas DataFrames using `pd.read_csv()` and `pd.read_excel()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2z0A4IC0Tj_"
      },
      "source": [
        "### Creating Dummy Data Files\n",
        "\n",
        "Before loading data, let's create some sample CSV and Excel files to work with. These files will contain simple tabular data with different data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc7c68e0"
      },
      "outputs": [],
      "source": [
        "# Create a dummy CSV file\n",
        "csv_data = {'ID': [1, 2, 3, 4, 5],\n",
        "            'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "            'Score': [85.5, 90.0, 78.2, 92.1, 88.9],\n",
        "            'Passed': [True, True, False, True, True]}\n",
        "df_csv_dummy = pd.DataFrame(csv_data)\n",
        "df_csv_dummy.to_csv('sample_data.csv', index=False)\n",
        "\n",
        "print(\"Dummy CSV file 'sample_data.csv' created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLj6XpBQ0O33"
      },
      "outputs": [],
      "source": [
        "# Create a dummy Excel file\n",
        "excel_data = {'ProductID': ['A101', 'B205', 'C303', 'D401', 'E509'],\n",
        "              'Price': [10.50, 25.75, 5.99, 100.00, 15.20],\n",
        "              'InStock': [50, 120, 200, 10, 75],\n",
        "              'Category': ['Electronics', 'Apparel', 'Groceries', 'Electronics', 'Apparel']}\n",
        "df_excel_dummy = pd.DataFrame(excel_data)\n",
        "df_excel_dummy.to_excel('another_sample_data.xlsx', index=False)\n",
        "\n",
        "print(\"Dummy Excel file 'another_sample_data.xlsx' created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53uzxebX0k-e"
      },
      "source": [
        "### Loading Data from Files\n",
        "\n",
        "Now that we have our dummy files, let's load the data into Pandas DataFrames\n",
        "using `pd.read_csv()` and `pd.read_excel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEa8ehP80xQ7"
      },
      "outputs": [],
      "source": [
        "# Load data from the CSV file\n",
        "df_csv = pd.read_csv('sample_data.csv')\n",
        "print(\"\\nData loaded from 'sample_data.csv' into df_csv.\")\n",
        "\n",
        "# Load data from the Excel file\n",
        "df_excel = pd.read_excel('another_sample_data.xlsx')\n",
        "print(\"Data loaded from 'another_sample_data.xlsx' into df_excel.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YZLadlC03Ei"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Initial Inspection of df_csv ---\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of df_csv:\")\n",
        "display(df_csv.head())\n",
        "\n",
        "print(\"\\nInformation about df_csv:\")\n",
        "display(df_csv.info())\n",
        "\n",
        "print(\"\\nDescriptive statistics of df_csv:\")\n",
        "display(df_csv.describe())\n",
        "\n",
        "print(\"\\n--- Initial Inspection of df_excel ---\")\n",
        "\n",
        "print(\"\\nFirst 5 rows of df_excel:\")\n",
        "display(df_excel.head())\n",
        "\n",
        "print(\"\\nInformation about df_excel:\")\n",
        "display(df_excel.info())\n",
        "\n",
        "print(\"\\nDescriptive statistics of df_excel:\")\n",
        "display(df_excel.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkm8uKOH1DDa"
      },
      "source": [
        "### Initial Data Inspection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "After loading the data, it's essential to perform an initial inspection to\n",
        "understand its structure, content, and basic statistics. We'll use the\n",
        "following methods:\n",
        "\n",
        "- `.head()`: Displays the first few rows of the DataFrame. Useful for a quick\n",
        "  look at the data's structure and values.\n",
        "- `.info()`: Provides a concise summary of the DataFrame, including the number\n",
        "  of non-null entries in each column and their data types. Helps identify missing\n",
        "  values and incorrect data types.\n",
        "- `.describe()`: Generates descriptive statistics for numerical columns, such as\n",
        "  count, mean, standard deviation, minimum, maximum, and quartiles. Useful for\n",
        "  understanding the distribution and range of numerical data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8e36797"
      },
      "source": [
        "## Data cleaning and preparation\n",
        "\n",
        "Data cleaning and preparation are critical steps in the data science workflow.\n",
        "Real-world datasets are often messy and contain errors, inconsistencies,\n",
        "missing values, and duplicates. Without proper cleaning, these issues can\n",
        "lead to inaccurate analyses, flawed models, and unreliable conclusions.\n",
        "\n",
        "This section will cover essential data cleaning techniques using Pandas:\n",
        "1.  Handling Missing Data: Identifying and dealing with missing values.\n",
        "2.  Handling Duplicates: Identifying and removing duplicate entries.\n",
        "3.  Data Type Conversion: Ensuring columns have the correct data types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7q5qIBncaq-"
      },
      "source": [
        "### Handling Missing Data\n",
        "\n",
        "Missing data is a common problem. It can occur due to various reasons,\n",
        "such as data collection errors, incomplete entries, or data corruption.\n",
        "Pandas represents missing values typically as `NaN` (Not a Number) for\n",
        "numerical data and `None` or `NaN` for object types.\n",
        "\n",
        "Identifying and handling missing data is crucial before performing any\n",
        "analysis or building models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bf5a011"
      },
      "outputs": [],
      "source": [
        "# Create a dummy DataFrame with missing values\n",
        "data_with_missing = {'A': [1, 2, np.nan, 4, 5],\n",
        "                     'B': [6, np.nan, 8, 9, 10],\n",
        "                     'C': ['X', 'Y', 'Z', np.nan, 'W'],\n",
        "                     'D': [11, 12, 13, 14, 15]}\n",
        "df_missing = pd.DataFrame(data_with_missing)\n",
        "\n",
        "print(\"Dummy DataFrame with missing values:\")\n",
        "display(df_missing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GySerBTydT1n"
      },
      "source": [
        "\n",
        "**Identifying Missing Values**\n",
        "\n",
        "We can identify missing values using the `.isnull()` method, which returns\n",
        "a boolean DataFrame of the same shape as the original DataFrame, where `True`\n",
        "indicates a missing value and `False` indicates a non-missing value.\n",
        "\n",
        "To get a count of missing values per column, we can chain `.isnull()` with\n",
        "`.sum()`. Since `True` is treated as 1 and `False` as 0 in numerical operations,\n",
        "`.sum()` will give us the total count of missing values in each column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlXqKM9JdQts"
      },
      "outputs": [],
      "source": [
        "print(\"\\nIdentifying missing values using .isnull():\")\n",
        "display(df_missing.isnull())\n",
        "\n",
        "print(\"\\nCounting missing values per column using .isnull().sum():\")\n",
        "display(df_missing.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPw8ZPnOdt96"
      },
      "source": [
        "**Handling Missing Values**\n",
        "\n",
        "Once missing values are identified, you need to decide how to handle them.\n",
        "Common strategies include:\n",
        "\n",
        "1.  *Dropping rows or columns:* Removing rows or columns that contain missing values.\n",
        "    - `.dropna()`: Removes rows or columns with missing values.\n",
        "2.  *Filling missing values:* Replacing missing values with a specified value\n",
        "    or using a strategy like mean, median, mode, forward fill, or backward fill.\n",
        "    - `.fillna()`: Fills missing values with a specified value or method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13ff9e19"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Handling Missing Values ---\")\n",
        "\n",
        "print(\"\\nOriginal DataFrame with missing values:\")\n",
        "display(df_missing)\n",
        "\n",
        "# Dropping rows with any missing values\n",
        "print(\"\\nDataFrame after dropping rows with any missing values (.dropna()):\")\n",
        "# Create a copy to avoid modifying the original DataFrame for subsequent examples\n",
        "df_dropped_rows = df_missing.dropna()\n",
        "display(df_dropped_rows)\n",
        "\n",
        "# Dropping columns with any missing values\n",
        "print(\"\\nDataFrame after dropping columns with any missing values (.dropna(axis=1)):\")\n",
        "df_dropped_cols = df_missing.dropna(axis=1)\n",
        "display(df_dropped_cols)\n",
        "\n",
        "# Filling missing values with a constant value\n",
        "print(\"\\nDataFrame after filling missing values in column 'A' with 0 (.fillna(0)):\")\n",
        "# Fill only in column 'A' for demonstration\n",
        "df_filled_const = df_missing.copy()\n",
        "df_filled_const['A'] = df_filled_const['A'].fillna(0)\n",
        "display(df_filled_const)\n",
        "\n",
        "# Filling missing values with the mean of the column\n",
        "print(\"\\nDataFrame after filling missing values in column 'B' with the mean (.fillna(df_missing['B'].mean())):\")\n",
        "df_filled_mean = df_missing.copy()\n",
        "df_filled_mean['B'] = df_filled_mean['B'].fillna(df_filled_mean['B'].mean())\n",
        "display(df_filled_mean)\n",
        "\n",
        "# Filling missing values with the median of the column\n",
        "print(\"\\nDataFrame after filling missing values in column 'B' with the median (.fillna(df_missing['B'].median())):\")\n",
        "df_filled_median = df_missing.copy()\n",
        "df_filled_median['B'] = df_filled_median['B'].fillna(df_filled_median['B'].median())\n",
        "display(df_filled_median)\n",
        "\n",
        "# Filling missing values using forward fill (propagating the last valid observation forward)\n",
        "print(\"\\nDataFrame after filling missing values using forward fill (.fillna(method='ffill')):\")\n",
        "df_filled_ffill = df_missing.copy()\n",
        "df_filled_ffill = df_filled_ffill.fillna(method='ffill')\n",
        "display(df_filled_ffill)\n",
        "\n",
        "# Filling missing values using backward fill (propagating the next valid observation backward)\n",
        "print(\"\\nDataFrame after filling missing values using backward fill (.fillna(method='bfill')):\")\n",
        "df_filled_bfill = df_missing.copy()\n",
        "df_filled_bfill = df_filled_bfill.fillna(method='bfill')\n",
        "display(df_filled_bfill)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "567492b7"
      },
      "source": [
        "**Handling Duplicate Data**\n",
        "\n",
        "Duplicate data occurs when two or more rows in a dataset are identical or\n",
        "nearly identical. Duplicates can skew analysis and lead to incorrect results,\n",
        "especially in calculations like counts or aggregations.\n",
        "\n",
        "Pandas provides methods to identify and remove duplicate rows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZiBD1UrM1Tl"
      },
      "outputs": [],
      "source": [
        "# Create a dummy DataFrame with duplicate rows\n",
        "data_with_duplicates = {'Col1': [1, 2, 3, 2, 4, 1],\n",
        "                        'Col2': ['A', 'B', 'C', 'B', 'D', 'A'],\n",
        "                        'Col3': [True, False, True, False, False, True]}\n",
        "df_duplicates = pd.DataFrame(data_with_duplicates)\n",
        "\n",
        "print(\"\\n--- Handling Duplicate Data ---\")\n",
        "\n",
        "print(\"\\nDummy DataFrame with duplicate rows:\")\n",
        "display(df_duplicates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m80c3AmXf1S9"
      },
      "source": [
        "**Identifying Duplicate Rows**\n",
        "The `.duplicated()` method returns a boolean Series indicating whether each\n",
        "row is a duplicate of a previous row. By default, it considers all columns\n",
        "to identify duplicates and marks the first occurrence as `False` and subsequent\n",
        "occurrences as `True`.\n",
        "\n",
        "- `keep='first'`: Marks duplicates as `True` except for the first occurrence. (Default)\n",
        "- `keep='last'`: Marks duplicates as `True` except for the last occurrence.\n",
        "- `keep=False`: Marks all duplicate occurrences as `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JEFmWSofy28"
      },
      "outputs": [],
      "source": [
        "print(\"\\nIdentifying duplicate rows using .duplicated() (keeping first occurrence as False):\")\n",
        "display(df_duplicates.duplicated())\n",
        "\n",
        "print(\"\\nIdentifying duplicate rows using .duplicated(keep='last') (keeping last occurrence as False):\")\n",
        "display(df_duplicates.duplicated(keep='last'))\n",
        "\n",
        "print(\"\\nIdentifying all duplicate occurrences using .duplicated(keep=False):\")\n",
        "display(df_duplicates.duplicated(keep=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBmr7UGbgQni"
      },
      "source": [
        "**Handling Duplicate Rows**\n",
        "\n",
        "The `.drop_duplicates()` method removes duplicate rows from the DataFrame.\n",
        "It has similar `keep` arguments as `.duplicated()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBWtmRy2gOSd"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRemoving duplicate rows using .drop_duplicates() (keeping the first occurrence):\")\n",
        "# Create a copy to avoid modifying the original DataFrame\n",
        "df_dropped_duplicates_first = df_duplicates.drop_duplicates(keep='first')\n",
        "display(df_dropped_duplicates_first)\n",
        "\n",
        "print(\"\\nRemoving duplicate rows using .drop_duplicates(keep='last') (keeping the last occurrence):\")\n",
        "df_dropped_duplicates_last = df_duplicates.drop_duplicates(keep='last')\n",
        "display(df_dropped_duplicates_last)\n",
        "\n",
        "print(\"\\nRemoving all duplicate occurrences using .drop_duplicates(keep=False):\")\n",
        "df_dropped_duplicates_all = df_duplicates.drop_duplicates(keep=False)\n",
        "display(df_dropped_duplicates_all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7iqpaSTgZdx"
      },
      "source": [
        "\n",
        "You can also identify and drop duplicates based on a subset of columns\n",
        "by passing a list of column names to the `subset` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZuKTjC_gczB"
      },
      "outputs": [],
      "source": [
        "print(\"\\nRemoving duplicate rows based on 'Col1' and 'Col2' using .drop_duplicates(subset=['Col1', 'Col2']):\")\n",
        "df_dropped_duplicates_subset = df_duplicates.drop_duplicates(subset=['Col1', 'Col2'])\n",
        "display(df_dropped_duplicates_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b57bec3a"
      },
      "source": [
        "**Data Type Conversion**\n",
        "\n",
        "Columns in a DataFrame can have different data types (e.g., integer, float,\n",
        "string, boolean, datetime). It's important that columns have the correct data\n",
        "types for accurate analysis and operations. For example, performing numerical\n",
        "calculations on a column that is incorrectly stored as a string will result in errors.\n",
        "\n",
        "The `.astype()` method is used to cast a Pandas object to a specified dtype.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a1244b1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Data Type Conversion ---\")\n",
        "\n",
        "print(\"\\nOriginal DataFrame (df_csv) and its data types:\")\n",
        "display(df_csv.head())\n",
        "display(df_csv.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXmcBYvfg-FR"
      },
      "source": [
        "Notice that the 'Passed' column is of type 'bool' and 'Score' is 'float64'.\n",
        "Let's say we want to convert the 'Score' column to an integer type and\n",
        "the 'Passed' column to a string type for some reason (e.g., for display\n",
        "or specific operations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5i7i0IZhFLM"
      },
      "outputs": [],
      "source": [
        "# Converting the 'Score' column to integer type\n",
        "# Note: This will truncate the decimal part. Be mindful of data loss.\n",
        "# If there were NaN values, you might need to fill them first or convert to a nullable integer type.\n",
        "try:\n",
        "    df_csv['Score_int'] = df_csv['Score'].astype(int)\n",
        "    print(\"\\nDataFrame after converting 'Score' column to integer ('Score_int'):\")\n",
        "    display(df_csv.head())\n",
        "    display(df_csv.info())\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not convert 'Score' to int directly due to potential NaNs or data type issues: {e}\")\n",
        "    # A more robust approach for potential NaNs is to convert to a nullable integer type\n",
        "    df_csv['Score_nullable_int'] = df_csv['Score'].astype('Int64')\n",
        "    print(\"\\nDataFrame after converting 'Score' column to nullable integer ('Score_nullable_int'):\")\n",
        "    display(df_csv.head())\n",
        "    display(df_csv.info())\n",
        "\n",
        "\n",
        "# Converting the 'Passed' column to string type\n",
        "df_csv['Passed_str'] = df_csv['Passed'].astype(str)\n",
        "print(\"\\nDataFrame after converting 'Passed' column to string ('Passed_str'):\")\n",
        "display(df_csv.head())\n",
        "display(df_csv.info())\n",
        "\n",
        "\"\"\"\n",
        "You can convert a column to various data types like 'float', 'int', 'bool',\n",
        "'str', 'datetime64[ns]', 'category', etc. It's important to ensure that the\n",
        "data in the column is compatible with the target data type.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nData Cleaning and Preparation Section Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bc2cae6"
      },
      "source": [
        "## Data Manipulation - Filtering, Sorting, Grouping, and Merging\n",
        "\n",
        "Data manipulation is at the heart of data analysis. It involves transforming,\n",
        "filtering, and restructuring your data to gain insights and prepare it\n",
        "for further analysis or modeling. Pandas provides a rich set of functions\n",
        "and methods for efficient data manipulation.\n",
        "\n",
        "In this section, we will cover four fundamental data manipulation techniques:\n",
        "1.  Filtering DataFrames: Selecting rows based on specific conditions.\n",
        "2.  Sorting DataFrames: Ordering rows based on the values in one or more columns.\n",
        "3.  Grouping Data: Splitting data into groups based on some criteria and\n",
        "    performing operations on those groups (aggregation).\n",
        "4.  Merging DataFrames: Combining multiple DataFrames based on common columns\n",
        "    or indices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO59WZJzhVsU"
      },
      "source": [
        "### Filtering DataFrames\n",
        "\n",
        "Filtering allows you to select a subset of rows from a DataFrame based on\n",
        "one or more conditions. This is typically done using boolean indexing, where\n",
        "you provide a boolean Series (or an array of booleans) to the DataFrame's\n",
        "index. The boolean Series should have the same length as the number of rows\n",
        "in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa7c6987"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Filtering DataFrames ---\")\n",
        "\n",
        "print(\"\\nOriginal DataFrame (df_csv):\")\n",
        "display(df_csv.head())\n",
        "\n",
        "# Filtering rows where 'Score' is greater than 85\n",
        "high_score_filter = df_csv['Score'] > 85\n",
        "df_high_scores = df_csv[high_score_filter]\n",
        "\n",
        "print(\"\\nRows where Score > 85:\")\n",
        "display(df_high_scores)\n",
        "\n",
        "# Filtering rows where 'Passed' is True\n",
        "passed_filter = df_csv['Passed'] == True\n",
        "df_passed = df_csv[passed_filter]\n",
        "\n",
        "print(\"\\nRows where Passed is True:\")\n",
        "display(df_passed)\n",
        "\n",
        "# Filtering with multiple conditions\n",
        "# To combine multiple conditions, use logical operators:\n",
        "# & (AND), | (OR), ~ (NOT)\n",
        "# Remember to wrap each condition in parentheses.\n",
        "high_score_and_passed_filter = (df_csv['Score'] > 85) & (df_csv['Passed'] == True)\n",
        "df_high_scores_and_passed = df_csv[high_score_and_passed_filter]\n",
        "\n",
        "print(\"\\nRows where Score > 85 AND Passed is True:\")\n",
        "display(df_high_scores_and_passed)\n",
        "\n",
        "# Filtering using the .query() method (useful for more complex conditions)\n",
        "# The .query() method uses string expressions to filter DataFrames.\n",
        "# It can sometimes be more readable for complex filtering logic.\n",
        "# Note: Variable names within the query string should be preceded by '@'.\n",
        "min_score = 88\n",
        "df_query_example = df_csv.query('Score > @min_score and Passed == True')\n",
        "print(f\"\\nRows where Score > {min_score} and Passed is True (using .query()):\")\n",
        "display(df_query_example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FvlGmnYh6pa"
      },
      "source": [
        "### Sorting DataFrames\n",
        "\n",
        "Sorting allows you to arrange the rows of a DataFrame in ascending or\n",
        "descending order based on the values in one or more columns. This is done\n",
        "using the `.sort_values()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJGjqS1Lhwfl"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Sorting DataFrames ---\")\n",
        "\n",
        "print(\"\\nOriginal DataFrame (df_csv):\")\n",
        "display(df_csv.head())\n",
        "\n",
        "# Sorting by a single column in ascending order (default)\n",
        "df_sorted_by_score_asc = df_csv.sort_values(by='Score')\n",
        "print(\"\\nDataFrame sorted by 'Score' (ascending):\")\n",
        "display(df_sorted_by_score_asc)\n",
        "\n",
        "# Sorting by a single column in descending order\n",
        "df_sorted_by_score_desc = df_csv.sort_values(by='Score', ascending=False)\n",
        "print(\"\\nDataFrame sorted by 'Score' (descending):\")\n",
        "display(df_sorted_by_score_desc)\n",
        "\n",
        "# Sorting by multiple columns\n",
        "# Sort by 'Passed' first (ascending), then by 'Score' (descending)\n",
        "df_sorted_multi = df_csv.sort_values(by=['Passed', 'Score'], ascending=[True, False])\n",
        "print(\"\\nDataFrame sorted by 'Passed' (asc) and then 'Score' (desc):\")\n",
        "display(df_sorted_multi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dON03tA9iLxn"
      },
      "source": [
        "### Grouping Data\n",
        "\n",
        "Grouping data is a powerful technique that involves splitting the data into\n",
        "groups based on some criteria, applying a function to each group independently,\n",
        "and combining the results into a DataFrame. The `.groupby()` method is used\n",
        "for this purpose. It's often followed by an aggregation function.\n",
        "\n",
        "Common aggregation functions include:\n",
        "- `sum()`: Compute sum of group values\n",
        "- `mean()`: Compute mean of group values\n",
        "- `median()`: Compute median of group values\n",
        "- `count()`: Compute count of group values (non-NaN)\n",
        "- `size()`: Compute group size (including NaN)\n",
        "- `min()`: Compute min of group values\n",
        "- `max()`: Compute max of group values\n",
        "- `std()`: Compute standard deviation of group values\n",
        "- `var()`: Compute variance of group values\n",
        "- `first()`: Compute first of group values\n",
        "- `last()`: Compute last of group values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y35fEk8tiLRT"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Grouping Data ---\")\n",
        "\n",
        "# Let's use the df_excel DataFrame for grouping demonstration\n",
        "print(\"\\nOriginal DataFrame (df_excel):\")\n",
        "display(df_excel.head())\n",
        "\n",
        "# Grouping by the 'Category' column and calculating the mean 'Price' for each category\n",
        "category_price_mean = df_excel.groupby('Category')['Price'].mean()\n",
        "print(\"\\nMean Price by Category:\")\n",
        "display(category_price_mean)\n",
        "\n",
        "# Grouping by 'Category' and calculating the sum of 'InStock'\n",
        "category_instock_sum = df_excel.groupby('Category')['InStock'].sum()\n",
        "print(\"\\nTotal InStock by Category:\")\n",
        "display(category_instock_sum)\n",
        "\n",
        "# Grouping by 'Category' and getting the size of each group\n",
        "category_counts = df_excel.groupby('Category').size()\n",
        "print(\"\\nNumber of products in each Category:\")\n",
        "display(category_counts)\n",
        "\n",
        "# Grouping by 'Category' and applying multiple aggregation functions\n",
        "category_agg = df_excel.groupby('Category').agg({\n",
        "    'Price': ['mean', 'min', 'max'],\n",
        "    'InStock': 'sum'\n",
        "})\n",
        "print(\"\\nAggregations (Mean Price, Min Price, Max Price, Sum InStock) by Category:\")\n",
        "display(category_agg)\n",
        "\n",
        "# Grouping by multiple columns\n",
        "# Let's add a dummy 'Location' column for demonstration\n",
        "df_excel['Location'] = ['A', 'B', 'A', 'B', 'A']\n",
        "print(\"\\nDataFrame df_excel with 'Location' column:\")\n",
        "display(df_excel.head())\n",
        "\n",
        "# Grouping by 'Category' and 'Location' and calculating the mean 'Price'\n",
        "category_location_price_mean = df_excel.groupby(['Category', 'Location'])['Price'].mean()\n",
        "print(\"\\nMean Price by Category and Location:\")\n",
        "display(category_location_price_mean)\n",
        "\n",
        "# Resetting the index after grouping to turn the group keys into columns\n",
        "category_location_price_mean_reset = category_location_price_mean.reset_index()\n",
        "print(\"\\nMean Price by Category and Location (with reset_index()):\")\n",
        "display(category_location_price_mean_reset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgqvcb0UiXwL"
      },
      "source": [
        "### Merging DataFrames\n",
        "\n",
        "Merging DataFrames is similar to joining tables in a relational database.\n",
        "It allows you to combine two DataFrames based on common columns or indices.\n",
        "The `pd.merge()` function is commonly used for this purpose.\n",
        "\n",
        "The key arguments for `pd.merge()` are:\n",
        "- `left`, `right`: The two DataFrames to merge.\n",
        "- `on`: The column(s) to join on. If not specified, it defaults to the\n",
        "    intersection of the columns in `left` and `right`.\n",
        "- `how`: The type of merge to perform.\n",
        "    - `'inner'` (default): Uses the intersection of keys from both DataFrames.\n",
        "    - `'outer'`: Uses the union of keys from both DataFrames, filling missing\n",
        "      values with `NaN`.\n",
        "    - `'left'`: Uses keys from the left DataFrame, keeping all rows from the left\n",
        "      and matching rows from the right.\n",
        "    - `'right'`: Uses keys from the right DataFrame, keeping all rows from the right\n",
        "      and matching rows from the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw2ziD8UiX8E"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Merging DataFrames ---\")\n",
        "\n",
        "# Create two dummy DataFrames for merging\n",
        "data1 = {'ID': [1, 2, 3, 4],\n",
        "         'Name': ['Alice', 'Bob', 'Charlie', 'David']}\n",
        "df1 = pd.DataFrame(data1)\n",
        "\n",
        "data2 = {'ID': [1, 2, 4, 5],\n",
        "         'Score': [85, 90, 92, 78]}\n",
        "df2 = pd.DataFrame(data2)\n",
        "\n",
        "print(\"\\nDataFrame 1 (df1):\")\n",
        "display(df1)\n",
        "\n",
        "print(\"\\nDataFrame 2 (df2):\")\n",
        "display(df2)\n",
        "\n",
        "# Inner merge (default): Keeps only rows where the key ('ID') exists in both DataFrames\n",
        "df_inner_merge = pd.merge(df1, df2, on='ID', how='inner')\n",
        "print(\"\\nInner Merge (on 'ID'):\")\n",
        "display(df_inner_merge)\n",
        "\n",
        "# Outer merge: Keeps all rows from both DataFrames, filling NaNs where no match\n",
        "df_outer_merge = pd.merge(df1, df2, on='ID', how='outer')\n",
        "print(\"\\nOuter Merge (on 'ID'):\")\n",
        "display(df_outer_merge)\n",
        "\n",
        "# Left merge: Keeps all rows from the left DataFrame (df1)\n",
        "df_left_merge = pd.merge(df1, df2, on='ID', how='left')\n",
        "print(\"\\nLeft Merge (on 'ID'):\")\n",
        "display(df_left_merge)\n",
        "\n",
        "# Right merge: Keeps all rows from the right DataFrame (df2)\n",
        "df_right_merge = pd.merge(df1, df2, on='ID', how='right')\n",
        "print(\"\\nRight Merge (on 'ID'):\")\n",
        "display(df_right_merge)\n",
        "\n",
        "# Merging on different column names\n",
        "# Create DataFrames with different key column names\n",
        "data3 = {'PersonID': [1, 2, 3],\n",
        "         'City': ['New York', 'London', 'Paris']}\n",
        "df3 = pd.DataFrame(data3)\n",
        "\n",
        "data4 = {'ID': [1, 2, 4],\n",
        "         'Salary': [50000, 60000, 70000]}\n",
        "df4 = pd.DataFrame(data4)\n",
        "\n",
        "print(\"\\nDataFrame 3 (df3):\")\n",
        "display(df3)\n",
        "print(\"\\nDataFrame 4 (df4):\")\n",
        "display(df4)\n",
        "\n",
        "# Merging df3 and df4 on 'PersonID' from df3 and 'ID' from df4\n",
        "df_merge_diff_keys = pd.merge(df3, df4, left_on='PersonID', right_on='ID', how='inner')\n",
        "print(\"\\nInner Merge (left_on='PersonID', right_on='ID'):\")\n",
        "display(df_merge_diff_keys)\n",
        "\n",
        "print(\"\\nData Manipulation Section Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99e70a49"
      },
      "source": [
        "## Data visualization basics with pandas\n",
        "\n",
        "Data visualization is a crucial step in the data analysis process. It allows\n",
        "us to understand the underlying patterns, distributions, and relationships\n",
        "within our data visually. While Matplotlib is the foundational plotting library\n",
        "in Python, Pandas provides a convenient wrapper around Matplotlib, allowing\n",
        "us to create basic plots directly from DataFrames and Series with minimal code.\n",
        "\n",
        "This section will introduce you to creating simple plots using the `.plot()`\n",
        "method available on Pandas objects, and other plotting methods like `.hist()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6z2iwoHvw_y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set a larger figure size for better readability of plots\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"--- Basic Data Visualization ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VisMrzIXvtt5"
      },
      "source": [
        "### Line Plot\n",
        "\n",
        "Line plots are useful for showing trends over time or ordered categories.\n",
        "They connect data points with lines. When used with a DataFrame or Series,\n",
        "the index is typically used for the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqHlAWdavt37"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCreating a simple Line Plot:\")\n",
        "# Let's use the 'Score' column from df_csv as a Series\n",
        "df_csv['Score'].plot(title='Student Scores (Line Plot)')\n",
        "plt.xlabel('Student Index')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "# Example with a DataFrame (if index represents a sequence)\n",
        "# For df_csv, the index is simply row number, so a line plot shows score progression by entry order.\n",
        "df_csv[['Score']].plot(kind='line', title='Student Scores Progression')\n",
        "plt.xlabel('Entry Order')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNoJxrxdv7JT"
      },
      "source": [
        "### Bar Plot\n",
        "\n",
        "Bar plots are effective for comparing quantities across different categories.\n",
        "They represent data with rectangular bars, where the length of each bar is\n",
        "proportional to the value it represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3nTC6OTv7Q6"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCreating a simple Bar Plot:\")\n",
        "# Using the 'Category' column from df_excel to show sum of 'InStock'\n",
        "category_instock_sum.plot(kind='bar', title='Total InStock by Category')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Total InStock')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()\n",
        "\n",
        "# Using a DataFrame with categories as index (like category_agg)\n",
        "category_agg['Price']['mean'].plot(kind='bar', title='Mean Price by Category')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Mean Price')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwkJZLEowD5e"
      },
      "source": [
        "### Histogram\n",
        "\n",
        "Histograms are used to visualize the distribution of a single numerical variable.\n",
        "They divide the data into bins and count how many values fall into each bin,\n",
        "displaying these counts as bars. This helps in understanding the shape of\n",
        "the data distribution (e.g., normal, skewed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5CVPrcRwEBY"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCreating a simple Histogram:\")\n",
        "\n",
        "# Using the .hist() method directly on a DataFrame (plots histograms for all numerical columns)\n",
        "df_excel[['Price', 'InStock']].hist(bins=10, figsize=(12, 5))\n",
        "plt.suptitle('Histograms of Numerical Columns', y=1.02) # Add a title to the figure\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuiYFKa8wO2K"
      },
      "source": [
        "### Scatter Plot\n",
        "\n",
        "Scatter plots are used to visualize the relationship between two numerical\n",
        "variables. Each point on the plot represents an observation, with its position\n",
        "determined by the values of the two variables. They are useful for identifying\n",
        "correlations, clusters, or outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69rhVJhRwO9m"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCreating a simple Scatter Plot:\")\n",
        "# Using 'Price' and 'InStock' from df_excel to see their relationship\n",
        "df_excel.plot(kind='scatter', x='Price', y='InStock', title='Price vs InStock')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('InStock')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nBasic Data Visualization Section Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJpNFbVH-Zp_"
      },
      "source": [
        "## Interactive Data Visualization with Plotly\n",
        "\n",
        "\n",
        "While Matplotlib provides a solid foundation for static plots, **Plotly**\n",
        "is a powerful library for creating interactive, publication-quality\n",
        "visualizations directly in your notebook or for web applications. Plotly\n",
        "visualizations are rendered using HTML and JavaScript, allowing for features\n",
        "like hovering over data points to see details, zooming, panning, and\n",
        "toggling data series.\n",
        "\n",
        "Plotly has a Python API that integrates well with Pandas DataFrames, making\n",
        "it easy to create complex interactive plots from your data.\n",
        "\n",
        "In this section, we will introduce Plotly and demonstrate how to create\n",
        "basic interactive plot types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uadwWp3B71J"
      },
      "source": [
        "### Interactive Scatter Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0BaoydWBZEh"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "print(\"\\nCreating a simple Scatter Plot:\")\n",
        "\n",
        "# Using 'Price' and 'InStock' from df_excel to see their relationship\n",
        "fig = px.scatter(\n",
        "    df_excel,\n",
        "    x=\"Price\",\n",
        "    y=\"InStock\",\n",
        "    title=\"Price vs InStock\",\n",
        "    labels={\"Price\": \"Price\", \"InStock\": \"In Stock\"}\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nBasic Data Visualization Section Completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH_DrMxtB_zX"
      },
      "source": [
        "### Interactive Line Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uluXRzXnB_9U"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "print(\"\\nCreating a simple Line Plot:\")\n",
        "\n",
        "# First version: Series as a line plot (Score progression by index)\n",
        "fig1 = px.line(\n",
        "    df_csv,\n",
        "    y=\"Score\",\n",
        "    title=\"Student Scores (Line Plot)\",\n",
        "    labels={\"index\": \"Student Index\", \"Score\": \"Score\"}\n",
        ")\n",
        "fig1.show()\n",
        "\n",
        "# Second version: Line plot from DataFrame (progression by entry order)\n",
        "fig2 = px.line(\n",
        "    df_csv,\n",
        "    y=\"Score\",\n",
        "    title=\"Student Scores Progression\",\n",
        "    labels={\"index\": \"Entry Order\", \"Score\": \"Score\"}\n",
        ")\n",
        "fig2.show()\n",
        "\n",
        "print(\"\\nBasic Data Visualization Section Completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McemCKZtCY6E"
      },
      "source": [
        "### Interactive Bar Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcVKooTdCVJB"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "print(\"\\nCreating Histograms of Numerical Columns:\")\n",
        "\n",
        "# Price histogram\n",
        "fig1 = px.histogram(\n",
        "    df_excel,\n",
        "    x=\"Price\",\n",
        "    nbins=10,\n",
        "    title=\"Histogram of Price\",\n",
        "    labels={\"Price\": \"Price\"}\n",
        ")\n",
        "fig1.show()\n",
        "\n",
        "# InStock histogram\n",
        "fig2 = px.histogram(\n",
        "    df_excel,\n",
        "    x=\"InStock\",\n",
        "    nbins=10,\n",
        "    title=\"Histogram of InStock\",\n",
        "    labels={\"InStock\": \"In Stock\"}\n",
        ")\n",
        "fig2.show()\n",
        "\n",
        "print(\"\\nHistograms of Numerical Columns Completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77d516a8"
      },
      "source": [
        "# Conclusion and further learning\n",
        "\n",
        "This notebook provided a comprehensive introduction to the core functionalities of the Pandas library in Python, essential for data manipulation, analysis, and preparation in data science. We covered:\n",
        "\n",
        "-   **Pandas Basics:** Introduction to DataFrames and Series, creating these objects, and fundamental data selection and indexing using `.loc[]` and `.iloc[]`.\n",
        "-   **Data Loading and Inspection:** Reading data from common file formats like CSV and Excel, and using methods like `.head()`, `.info()`, and `.describe()` for initial data understanding.\n",
        "-   **Data Cleaning and Preparation:** Identifying and handling missing values (`.isnull()`, `.dropna()`, `.fillna()`), identifying and removing duplicate data (`.duplicated()`, `.drop_duplicates()`), and converting data types (`.astype()`).\n",
        "-   **Data Manipulation:** Filtering data based on conditions, sorting DataFrames by values, grouping data for aggregation (`.groupby()`, `.agg()`), and combining DataFrames through merging (`pd.merge()`).\n",
        "-   **Basic Data Visualization:** Creating common plots directly from Pandas objects using the `.plot()` method (line plots, bar plots, scatter plots) and generating histograms (`.hist()`) to visualize data distributions.\n",
        "\n",
        "\n",
        "Mastering these Pandas concepts is fundamental for anyone pursuing a career in data science. The ability to efficiently load, clean, transform, and analyze data is a cornerstone skill.\n",
        "\n",
        "---\n",
        "\n",
        "**Resources for Continued Learning**\n",
        "\n",
        "While this notebook covers major Pandas topics, the journey of learning data science tools is continuous. Here are some resources to deepen your understanding and expand your skills:\n",
        "\n",
        "1.  **Official Pandas Documentation:** The most authoritative and comprehensive resource. It provides detailed explanations, examples, and API references. [https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/)\n",
        "2.  **Matplotlib and plotly Documentation:** For more advanced and aesthetically pleasing visualizations, explore these libraries.\n",
        "    -   Matplotlib: [https://matplotlib.org/stable/tutorials/index](https://matplotlib.org/stable/tutorials/index)\n",
        "    - Plotly: [https://plotly.com/python/](https://plotly.com/python/)\n",
        "4.  **Scikit-learn Documentation:** For machine learning tasks, Scikit-learn is a key library that often works with Pandas DataFrames. [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "5.  **Online Courses:** Platforms like Coursera, edX, DataCamp, and Udacity offer specialized courses on Python for Data Science and Pandas.\n",
        "6.  **Books:** Many excellent books cover Python and Pandas for data analysis, such as \"Python for Data Analysis\" by Wes McKinney (the creator of Pandas).\n",
        "7.  **Kaggle:** Practice your skills on real-world datasets and learn from others' notebooks. [https://www.kaggle.com/](https://www.kaggle.com/)\n",
        "8.  **Towards Data Science & Medium:** Blogs and articles from the data science community often provide practical tutorials and insights.\n",
        "\n",
        "Keep practicing with different datasets and projects to reinforce your learning. The more you work with data using Pandas, the more proficient you will become.\n",
        "\n",
        "Good luck on your data science journey!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42fac77a"
      },
      "source": [
        "# Numpy basics\n",
        "\n",
        "Welcome to the section on NumPy (Numerical Python), a fundamental library for\n",
        "numerical computing in Python. NumPy is the backbone of many other scientific\n",
        "libraries in Python, including Pandas, Scikit-learn, and SciPy.\n",
        "\n",
        "**What is NumPy?**\n",
        "NumPy provides support for large, multi-dimensional arrays and matrices, along\n",
        "with a large collection of high-level mathematical functions to operate on\n",
        "these arrays. Its core object is the `ndarray` (n-dimensional array), which\n",
        "is a fast and flexible container for homogeneous data of arbitrary dimensions.\n",
        "\n",
        "**Why use NumPy?**\n",
        "- **Efficiency:** NumPy arrays are much faster and more memory-efficient than\n",
        "  standard Python lists for numerical operations, especially for large datasets.\n",
        "  This is because NumPy arrays are implemented in C, allowing for vectorized\n",
        "  operations that avoid the overhead of Python loops.\n",
        "- **Functionality:** NumPy provides a wide range of mathematical functions\n",
        "  optimized for array operations, including linear algebra, Fourier analysis,\n",
        "  and random number generation.\n",
        "- **Foundation:** Many other data science libraries rely on NumPy arrays as\n",
        "  their primary data structure.\n",
        "\n",
        "Let's start by exploring how to create NumPy arrays.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS7Z0HKx9sK3"
      },
      "source": [
        "### Creating NumPy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c12a33d3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"--- Introduction to NumPy ---\")\n",
        "\n",
        "\"\"\"\n",
        "There are several ways to create NumPy arrays:\n",
        "\"\"\"\n",
        "\n",
        "# X.1.1 Creating arrays from Python lists or nested lists\n",
        "\n",
        "# Creating a 1-dimensional array (vector) from a list\n",
        "list_1d = [1, 2, 3, 4, 5]\n",
        "numpy_array_1d = np.array(list_1d)\n",
        "print(\"\\n1D NumPy array from a list:\")\n",
        "display(numpy_array_1d)\n",
        "print(\"Shape:\", numpy_array_1d.shape)\n",
        "print(\"Data type:\", numpy_array_1d.dtype)\n",
        "\n",
        "# Creating a 2-dimensional array (matrix) from a nested list\n",
        "list_2d = [[1, 2, 3], [4, 5, 6]]\n",
        "numpy_array_2d = np.array(list_2d)\n",
        "print(\"\\n2D NumPy array from a nested list:\")\n",
        "display(numpy_array_2d)\n",
        "print(\"Shape:\", numpy_array_2d.shape)\n",
        "print(\"Data type:\", numpy_array_2d.dtype)\n",
        "\n",
        "# X.1.2 Creating arrays using built-in NumPy functions\n",
        "\n",
        "# Creating an array of zeros\n",
        "zeros_array = np.zeros((3, 4)) # 3 rows, 4 columns\n",
        "print(\"\\nArray of zeros (3x4):\")\n",
        "display(zeros_array)\n",
        "\n",
        "# Creating an array of ones\n",
        "ones_array = np.ones((2, 3)) # 2 rows, 3 columns\n",
        "print(\"\\nArray of ones (2x3):\")\n",
        "display(ones_array)\n",
        "\n",
        "# Creating an array with a sequence of numbers (like Python's range)\n",
        "arange_array = np.arange(10) # from 0 to 9\n",
        "print(\"\\nArray using np.arange(10):\")\n",
        "display(arange_array)\n",
        "\n",
        "arange_start_stop = np.arange(2, 10, 2) # from 2 to 9 with step 2\n",
        "print(\"\\nArray using np.arange(2, 10, 2):\")\n",
        "display(arange_start_stop)\n",
        "\n",
        "# Creating an array with a specified number of evenly spaced values\n",
        "linspace_array = np.linspace(0, 1, 5) # 5 points between 0 and 1 (inclusive)\n",
        "print(\"\\nArray using np.linspace(0, 1, 5):\")\n",
        "display(linspace_array)\n",
        "\n",
        "# X.1.3 Creating arrays with random values\n",
        "\n",
        "# Creating an array with random values from a uniform distribution [0, 1)\n",
        "random_uniform = np.random.rand(3, 2) # 3 rows, 2 columns\n",
        "print(\"\\nArray with random values from uniform distribution (3x2):\")\n",
        "display(random_uniform)\n",
        "\n",
        "# Creating an array with random values from a standard normal distribution (mean 0, variance 1)\n",
        "random_normal = np.random.randn(2, 3) # 2 rows, 3 columns\n",
        "print(\"\\nArray with random values from standard normal distribution (2x3):\")\n",
        "display(random_normal)\n",
        "\n",
        "# Creating an array of random integers within a specified range\n",
        "random_integers = np.random.randint(1, 10, size=(2, 5)) # integers between 1 (inclusive) and 10 (exclusive), size 2x5\n",
        "print(\"\\nArray with random integers (1-9, size 2x5):\")\n",
        "display(random_integers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef586b9"
      },
      "source": [
        "### NumPy Array Indexing and Slicing\n",
        "\n",
        "Accessing elements or subsets of NumPy arrays is similar to Python lists\n",
        "but with extended capabilities for multi-dimensional arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e23c1fa"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- NumPy Array Indexing and Slicing ---\")\n",
        "\n",
        "# Using the 1D array: numpy_array_1d\n",
        "print(\"\\nOriginal 1D array:\")\n",
        "display(numpy_array_1d)\n",
        "\n",
        "# Accessing elements using integer indices\n",
        "print(\"\\nAccessing the first element:\", numpy_array_1d[0])\n",
        "print(\"Accessing the last element:\", numpy_array_1d[-1])\n",
        "\n",
        "# Slicing 1D arrays\n",
        "print(\"\\nSlicing the first three elements:\", numpy_array_1d[0:3]) # or numpy_array_1d[:3]\n",
        "print(\"Slicing elements from index 2 onwards:\", numpy_array_1d[2:])\n",
        "print(\"Slicing with a step:\", numpy_array_1d[::2]) # Every second element\n",
        "\n",
        "\n",
        "# Using the 2D array: numpy_array_2d\n",
        "print(\"\\nOriginal 2D array:\")\n",
        "display(numpy_array_2d)\n",
        "\n",
        "# Accessing elements in 2D arrays (row, column)\n",
        "print(\"\\nAccessing the element at row 0, column 1:\", numpy_array_2d[0, 1])\n",
        "print(\"Accessing the element at row 1, column 2:\", numpy_array_2d[1, 2])\n",
        "\n",
        "# Slicing 2D arrays (row_slice, col_slice)\n",
        "print(\"\\nSlicing the first row:\", numpy_array_2d[0, :]) # or numpy_array_2d[0]\n",
        "print(\"Slicing the second column:\", numpy_array_2d[:, 1])\n",
        "print(\"Slicing the top-left 2x2 sub-array:\\n\", numpy_array_2d[0:2, 0:2])\n",
        "\n",
        "\n",
        "# X.2.3 Boolean Indexing\n",
        "\n",
        "\"\"\"\n",
        "Boolean indexing allows you to select elements from an array based on a\n",
        "boolean condition. The condition returns a boolean array of the same shape\n",
        "as the original array, and only elements where the boolean array is True are selected.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Boolean Indexing ---\")\n",
        "\n",
        "# Using the 1D array\n",
        "print(\"\\nOriginal 1D array:\", numpy_array_1d)\n",
        "# Select elements greater than 3\n",
        "boolean_condition = numpy_array_1d > 3\n",
        "print(\"Boolean condition (array > 3):\", boolean_condition)\n",
        "print(\"Elements greater than 3:\", numpy_array_1d[boolean_condition])\n",
        "\n",
        "# Using the 2D array\n",
        "print(\"\\nOriginal 2D array:\\n\", numpy_array_2d)\n",
        "# Select elements that are even\n",
        "even_condition = numpy_array_2d % 2 == 0\n",
        "print(\"Boolean condition (even elements):\\n\", even_condition)\n",
        "print(\"Even elements:\", numpy_array_2d[even_condition]) # Returns a 1D array of selected elements\n",
        "\n",
        "\n",
        "# X.2.4 Fancy Indexing (Integer Array Indexing)\n",
        "\n",
        "\"\"\"\n",
        "Fancy indexing allows you to select arbitrary elements or rows/columns\n",
        "using arrays of integers.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Fancy Indexing ---\")\n",
        "\n",
        "# Using the 1D array\n",
        "print(\"\\nOriginal 1D array:\", numpy_array_1d)\n",
        "# Select elements at indices 0, 4, and 2\n",
        "indices_to_select = [0, 4, 2]\n",
        "print(\"Elements at indices [0, 4, 2]:\", numpy_array_1d[indices_to_select])\n",
        "\n",
        "# Using the 2D array\n",
        "print(\"\\nOriginal 2D array:\\n\", numpy_array_2d)\n",
        "# Select rows at indices 1 and 0\n",
        "rows_to_select = [1, 0]\n",
        "print(\"Selecting rows at indices [1, 0]:\\n\", numpy_array_2d[rows_to_select, :]) # or numpy_array_2d[rows_to_select]\n",
        "\n",
        "# Select elements at (0,0), (1,1), and (1,2)\n",
        "row_indices = [0, 1, 1]\n",
        "col_indices = [0, 1, 2]\n",
        "print(\"Selecting elements at (0,0), (1,1), and (1,2):\", numpy_array_2d[row_indices, col_indices]) # Returns a 1D array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cc1d6d8"
      },
      "source": [
        "### Basic NumPy Operations\n",
        "\n",
        "NumPy arrays support a wide range of operations, many of which are performed\n",
        "element-wise and are significantly faster than equivalent operations on Python lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd5f522a"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Basic NumPy Operations ---\")\n",
        "\n",
        "# Using the 1D array: numpy_array_1d\n",
        "print(\"\\nOriginal 1D array:\", numpy_array_1d)\n",
        "\n",
        "# X.3.1 Element-wise Arithmetic Operations\n",
        "\n",
        "# Addition\n",
        "array_addition = numpy_array_1d + 5\n",
        "print(\"\\nAdding a scalar (5) to the array:\", array_addition)\n",
        "\n",
        "# Subtraction\n",
        "array_subtraction = numpy_array_1d - 2\n",
        "print(\"Subtracting a scalar (2) from the array:\", array_subtraction)\n",
        "\n",
        "# Multiplication\n",
        "array_multiplication = numpy_array_1d * 3\n",
        "print(\"Multiplying the array by a scalar (3):\", array_multiplication)\n",
        "\n",
        "# Division\n",
        "array_division = numpy_array_1d / 2\n",
        "print(\"Dividing the array by a scalar (2):\", array_division)\n",
        "\n",
        "# Element-wise operations between two arrays of the same shape\n",
        "array_1 = np.array([1, 2, 3])\n",
        "array_2 = np.array([4, 5, 6])\n",
        "array_sum = array_1 + array_2\n",
        "print(\"\\nElement-wise addition of two arrays:\", array_sum)\n",
        "\n",
        "\n",
        "# X.3.2 Array Broadcasting\n",
        "\n",
        "\"\"\"\n",
        "Broadcasting is a powerful feature in NumPy that allows arithmetic operations\n",
        "between arrays with different shapes. When the shapes are compatible, NumPy\n",
        "\"broadcasts\" the smaller array across the larger array so that they have\n",
        "compatible shapes for the operation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Array Broadcasting ---\")\n",
        "\n",
        "# Example: Adding a 1D array (row vector) to a 2D array\n",
        "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "vector = np.array([10, 20, 30])\n",
        "\n",
        "print(\"\\nMatrix (2x3):\\n\", matrix)\n",
        "print(\"Vector (1x3):\", vector)\n",
        "\n",
        "# NumPy broadcasts the vector across the rows of the matrix\n",
        "broadcasted_sum = matrix + vector\n",
        "print(\"Result of broadcasting (Matrix + Vector):\\n\", broadcasted_sum)\n",
        "\n",
        "# Example: Adding a 1D array (column vector - needs reshaping) to a 2D array\n",
        "column_vector = np.array([[100], [200]])\n",
        "print(\"\\nColumn Vector (2x1):\\n\", column_vector)\n",
        "\n",
        "# NumPy broadcasts the column vector across the columns of the matrix\n",
        "broadcasted_sum_col = matrix + column_vector\n",
        "print(\"Result of broadcasting (Matrix + Column Vector):\\n\", broadcasted_sum_col)\n",
        "\n",
        "\n",
        "# X.3.3 Basic Aggregation Functions\n",
        "\n",
        "\"\"\"\n",
        "NumPy provides functions to compute summary statistics on arrays.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Basic Aggregation Functions ---\")\n",
        "\n",
        "# Using the 1D array\n",
        "print(\"\\nOriginal 1D array:\", numpy_array_1d)\n",
        "print(\"Sum of elements:\", numpy_array_1d.sum())\n",
        "print(\"Mean of elements:\", numpy_array_1d.mean())\n",
        "print(\"Maximum element:\", numpy_array_1d.max())\n",
        "print(\"Minimum element:\", numpy_array_1d.min())\n",
        "print(\"Standard deviation:\", numpy_array_1d.std())\n",
        "\n",
        "# Using the 2D array\n",
        "print(\"\\nOriginal 2D array:\\n\", numpy_array_2d)\n",
        "print(\"Sum of all elements:\", numpy_array_2d.sum())\n",
        "print(\"Mean of all elements:\", numpy_array_2d.mean())\n",
        "\n",
        "# Aggregations along an axis\n",
        "print(\"\\nSum along axis 0 (columns):\", numpy_array_2d.sum(axis=0)) # Sum of each column\n",
        "print(\"Mean along axis 1 (rows):\", numpy_array_2d.mean(axis=1)) # Mean of each row\n",
        "\n",
        "\n",
        "# X.3.4 Matrix Operations\n",
        "\n",
        "\"\"\"\n",
        "NumPy is widely used for linear algebra operations. The dot product is a\n",
        "fundamental matrix operation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Matrix Operations (Dot Product) ---\")\n",
        "\n",
        "# Using two 2D arrays (matrices)\n",
        "matrix_a = np.array([[1, 2], [3, 4]])\n",
        "matrix_b = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "print(\"\\nMatrix A (2x2):\\n\", matrix_a)\n",
        "print(\"Matrix B (2x2):\\n\", matrix_b)\n",
        "\n",
        "# Matrix multiplication (dot product)\n",
        "# The number of columns in the first matrix must equal the number of rows in the second matrix.\n",
        "dot_product_result = np.dot(matrix_a, matrix_b)\n",
        "print(\"Dot product of Matrix A and Matrix B:\\n\", dot_product_result)\n",
        "\n",
        "# Another way to compute dot product using the @ operator (Python 3.5+)\n",
        "dot_product_at = matrix_a @ matrix_b\n",
        "print(\"Dot product using @ operator:\\n\", dot_product_at)\n",
        "\n",
        "print(\"\\nNumPy Basics Section Completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f37fd17"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "\n",
        "Now that the major topics are covered, I will formulate hands-on exercises for each relevant section as per the instructions. This involves pandas, numpy and visualization (Plotly, matplotlib).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dg-a5qn19eN"
      },
      "source": [
        "## Exercise 1 (after Pandas basics)\n",
        "\n",
        "**Exercise 1: Pandas Basics**\n",
        "\n",
        "Using the `dataframe_from_dict` created in Section 2, perform the following operations:\n",
        "1. Select and display the 'Age' column using both dot notation and bracket notation.\n",
        "2. Select and display the row with index label 2 using `.loc[]`.\n",
        "3. Select and display the 'Name' and 'City' columns for the first two rows using `.iloc[]`.\n",
        "4. Select and display the 'Age' and 'City' columns for the row where the 'Name' is 'David' using `.loc[]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WJtN3dl2BDS"
      },
      "outputs": [],
      "source": [
        "# Your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0jwDBM22EJC"
      },
      "source": [
        "## Exercise 2 (after Data loading and inspection)\n",
        "\n",
        "**Exercise 2: Data Loading and Inspection**\n",
        "\n",
        "Load the 'sample_data.csv' file (created in Section 3) into a new DataFrame called `students_df`.\n",
        "Then, perform the following inspection steps:\n",
        "1. Display the first 3 rows of `students_df`.\n",
        "2. Display a concise summary of the DataFrame, including data types and non-null counts.\n",
        "3. Display descriptive statistics for the numerical columns in `students_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzACOHUK2ERU"
      },
      "outputs": [],
      "source": [
        "# Your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BZd3xaa2iVI"
      },
      "source": [
        "## Exercise 3 (after Data cleaning and preparation)\n",
        "\n",
        "**Exercise 3: Data Cleaning and Preparation**\n",
        "\n",
        "Using the `df_missing` DataFrame created in Section 4, perform the following cleaning tasks:\n",
        "1. Count the number of missing values in each column.\n",
        "2. Drop rows that have at least one missing value and display the resulting DataFrame.\n",
        "3. Fill the missing values in column 'A' with the mean of column 'A' and display the resulting DataFrame.\n",
        "4. Fill the missing values in column 'C' with the string 'Missing' and display the resulting DataFrame.\n",
        "\n",
        "Using the `df_duplicates` DataFrame created in Section 4:\n",
        "5. Identify and display all duplicate rows (marking all occurrences of duplicates as True).\n",
        "6. Drop duplicate rows based on the 'Col2' column, keeping the last occurrence, and display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYaGNYMF2ije"
      },
      "outputs": [],
      "source": [
        "# Your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6mVXGX82yn2"
      },
      "source": [
        "## Exercise 4 (after Data manipulation)\n",
        "\n",
        "**Exercise 4: Data Manipulation**\n",
        "\n",
        "Using the `df_csv` DataFrame (loaded and potentially modified in previous sections), perform the following manipulations:\n",
        "1. Filter and display rows where the 'Score' is less than 80.\n",
        "2. Sort the DataFrame by 'Score' in descending order and display the result.\n",
        "3. Calculate the mean 'Score' for students who 'Passed' and those who did not.\n",
        "4. Merge the `df1` and `df2` DataFrames (created in Section 5) using a left merge on the 'ID' column and display the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GxFq43e2yvB"
      },
      "outputs": [],
      "source": [
        "# Your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crYsX6zH22rk"
      },
      "source": [
        "## Exercise 5 (after Data visualization basics)\n",
        "\n",
        "**Exercise 5: Basic Data Visualization**\n",
        "\n",
        "Using the `df_excel` DataFrame (loaded and potentially modified in previous sections), create the following visualizations using matplotlib and plotly:\n",
        "1. Create a bar plot showing the total 'InStock' quantity for each 'Category'.\n",
        "2. Create a histogram of the 'Price' column with 10 bins.\n",
        "3. Create a scatter plot showing the relationship between 'Price' and 'InStock'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVtaLDK6Rjm5"
      },
      "outputs": [],
      "source": [
        "# Your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c7d1313"
      },
      "source": [
        "## Exercise 6 (after Numpy basics)\n",
        "\n",
        "**Exercise 6: NumPy Basics**\n",
        "\n",
        "Perform the following operations using NumPy:\n",
        "1. Create a 1D NumPy array named `numpy_exercise_array` with values from 10 to 20 (inclusive).\n",
        "2. Access and display the element at index 5 of `numpy_exercise_array`.\n",
        "3. Slice `numpy_exercise_array` to get elements from index 3 to 7 (inclusive) and display the result.\n",
        "4. Create a 2x3 NumPy array named `numpy_matrix` with random integers between 1 and 10.\n",
        "5. Access and display the element at row 1, column 2 of `numpy_matrix`.\n",
        "6. Calculate and display the mean of all elements in `numpy_matrix`.\n",
        "7. Calculate and display the sum of elements along axis 0 (columns) of `numpy_matrix`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d314a5f"
      },
      "source": [
        "## Exercise 7 (Comprehensive)\n",
        "\n",
        "\n",
        "This exercise will test your understanding of the key concepts covered in this notebook, including Pandas for data manipulation, NumPy for numerical operations, and Matplotlib/Plotly for data visualization.\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "You are a data analyst exploring the California Housing dataset (https://www.kaggle.com/datasets/camnugent/california-housing-prices/data), which contains information about housing prices and other related features in California districts.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1.  **Data Loading and Inspection (Pandas):**\n",
        "    *   Load the `california_housing_train.csv` file into a Pandas DataFrame named `housing_data`.\n",
        "    *   Display the first 5 rows, the data types of each column, and descriptive statistics for the numerical columns in `housing_data`.\n",
        "\n",
        "2.  **Data Cleaning and Preparation (Pandas):**\n",
        "    *   Introduce a few missing values into the `median_house_value` column of `housing_data` (e.g., set the value for a couple of rows to `np.nan`).\n",
        "    *   Count the number of missing values in the `median_house_value` column.\n",
        "    *   Fill the missing values in the `median_house_value` column with the mean of the non-missing values.\n",
        "    *   Introduce a few duplicate rows into `housing_data` by concatenating a few existing rows.\n",
        "    *   Identify and display the duplicate rows.\n",
        "    *   Remove the duplicate rows, keeping the first occurrence.\n",
        "\n",
        "3.  **Data Manipulation (Pandas):**\n",
        "    *   Filter `housing_data` to create a new DataFrame `expensive_houses` containing houses with `median_house_value` greater than the overall median house value.\n",
        "    *   Sort `expensive_houses` by `median_income` in descending order.\n",
        "    *   Group `housing_data` by `ocean_proximity` and calculate the average `median_income` for each group.\n",
        "\n",
        "4.  **NumPy Operations:**\n",
        "    *   Extract the `median_income` column from the original `housing_data` DataFrame (before adding NaNs or duplicates) as a NumPy array named `income_array`.\n",
        "    *   Calculate and display the mean, median, and standard deviation of `income_array` using NumPy functions.\n",
        "    *   Create a 3x3 NumPy array with random floating-point values between 0 and 1.\n",
        "    *   Calculate the inverse of this 3x3 array using NumPy's linear algebra module.\n",
        "\n",
        "5.  **Data Visualization (Matplotlib and Plotly):**\n",
        "    *   Using Matplotlib, create a histogram of the `median_house_value` column from the cleaned `housing_data` DataFrame.\n",
        "    *   Using Plotly, create an interactive scatter plot of `median_income` vs `median_house_value` from the cleaned `housing_data` DataFrame.\n",
        "    *   Using Plotly, create an interactive bar plot showing the average `median_house_value` for each `ocean_proximity` category (use the results from the grouping in Task 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "765q9f5biwNw"
      },
      "outputs": [],
      "source": [
        "# Your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd26b722"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Feel free to ask any questions about the concepts covered or the exercises! I'm here to help you understand Pandas, NumPy, and data visualization better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPu8Bx3hS3A5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
